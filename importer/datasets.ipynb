{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "<div style=\"position: absolute; right:0;top:0\"><a href=\"./importer.ipynb\" style=\"text-decoration: none\"> <font size=\"5\">←</font></a>\n",
    "<a href=\"../evaluation.py.ipynb\" style=\"text-decoration: none\"> <font size=\"5\">↑</font></a></div>\n",
    "\n",
    "This is an overview of all currently supported datasets. Datasets marked with ° contain ground truth class labels.\n",
    "Datasets marked with * are currently not working.\n",
    "\n",
    "- [20 Newsgroup](#20-Newsgroup)°\n",
    "- [ACM](#ACM)\n",
    "- [ATD](#ATD)\n",
    "- [Classic4](#Classic4)°\n",
    "- [DBLP](#DBLP)*\n",
    "- [L5 - Yahoo! Answers Manner Questions](#L5---Yahoo!-Answers-Manner-Questions)°\n",
    "- [Reuters](#Reuters)°\n",
    "- [TweetsLA](#TweetsLA)\n",
    "- [TweetsODP](#TweetsODP)°\n",
    "- [US Consumer Finance Complaints](#US-Consumer-Finance-Complaints)°\n",
    "\n",
    "---\n",
    "\n",
    "## 20 Newsgroup\n",
    "Identifier | newsgroup\n",
    "-----------|-------\n",
    "Importer   | [NewsgroupImporter](./newsgroup.py)\n",
    "Source     | https://scikit-learn.org/stable/datasets/index.html#newsgroups-dataset\n",
    "Documents  | 18846\n",
    "Classes    | 20\n",
    "\n",
    "Data will be downloaded to `data/raw/sklearn` automatically by the import script. Parameters:\n",
    "- `remove` (list of strings)  \n",
    "  Possible values are: `headers`, `footers`, `quotes`.  \n",
    "  See https://scikit-learn.org/0.19/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups\n",
    "\n",
    "\n",
    "## ACM\n",
    "Identifier | acm\n",
    "-----------|-------\n",
    "Importer   | [ACMImporter](./acm.py)\n",
    "Source     | https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27695\n",
    "Documents  | 36396\n",
    "Classes    | -\n",
    "\n",
    "Download the abstract.zip file to `data/raw/acm`.\n",
    "\n",
    "\n",
    "## ATD\n",
    "Identifier | atd\n",
    "-----------|-------\n",
    "Importer   | [ATDImporter](./atd.py)\n",
    "Source     | various\n",
    "Documents  | 105\n",
    "Classes    | -\n",
    "\n",
    "Originally 105 files of text extracted from publications of the participants of the ATD conference in Washington D.C. 2018. It is not publicly available, but you may use this class as a generic dataset. To do so, put .txt files in the `data/raw/atd` folder, one file per document. It does not support classes.\n",
    "\n",
    "\n",
    "## Classic4\n",
    "Identifier | classic4\n",
    "-----------|-------\n",
    "Importer   | [ClassicImporter](./classic4.py)\n",
    "Source     | http://www.dataminingresearch.com/index.php/2010/09/classic3-classic4-datasets/\n",
    "Documents  | 7095\n",
    "Classes    | 4\n",
    "\n",
    "Download the classicdocs.rar (\"*You can freely download the whole collection (1.5MB RAR file).*\") and extract it. \n",
    "All files should be in the `data/raw/classic4` folder.\n",
    "Files are named according to their class (\"cacm\",\"cisi\",\"cran\",\"med\") plus an integer.\n",
    "\n",
    "## DBLP\n",
    "Identifier | dblp\n",
    "-----------|-------\n",
    "Source     | http://dblp.org/xml/release/\n",
    "Documents  | ?\n",
    "Classes    | ?\n",
    "\n",
    "Download and extract `dblp-2018-11-01.xml` to `data/raw/dblp`.\n",
    "\n",
    "\n",
    "\n",
    "## L5 - Yahoo! Answers Manner Questions \n",
    "Identifier | yahooL5\n",
    "-----------|-------\n",
    "Importer   | [YahooImporter](./yahoo.py)\n",
    "Source     | https://webscope.sandbox.yahoo.com/catalog.php?datatype=l&did=10\n",
    "Documents  | 142594\n",
    "Classes    | 24\n",
    "Version    | 2.0\n",
    "\n",
    "Request and download the Webscope_L5.tgz file. Extract and copy `manner.xml` to `data/raw/yahooL5`.\n",
    "\n",
    "\n",
    "## Reuters\n",
    "Identifier | reuters\n",
    "-----------|-------\n",
    "Importer   | [ReutersImporter](./reuters.py)\n",
    "Source     | https://www.nltk.org/data.html\n",
    "Documents  | 10788\n",
    "Classes    | 90\n",
    "\n",
    "This dataset can be downloaded with the *Natural Language Toolkit*. Set `data/raw/nltk` as the download directory when following these [instructions](https://www.nltk.org/data.html).\n",
    "In short:\n",
    "1. Run `python -m nltk.downloader` from your environment.\n",
    "2. Set `data/raw/nltk` as the download directory.\n",
    "3. From `corpora` select `reuters`.\n",
    "4. Press Download.\n",
    "\n",
    "Parameters:\n",
    "- `min_docs_per_class` (int)  \n",
    "  Only keep classes (and corresponding documents) containing at least `min_docs_per_class` documents.\n",
    "\n",
    "\n",
    "## TweetsLA\n",
    "Identifier | tweetsla\n",
    "-----------|-------\n",
    "Importer   | [TweetsLAImporter](./tweetsla.py)\n",
    "Source     | Twitter API\n",
    "Documents  | ~90.000 per day\n",
    "Classes    | -\n",
    "\n",
    "Put one or multiple .zip files in the `data/raw/tweetsla` folder.\n",
    "Each archive should contain one or multiple text files where each line is a tweet in json format.\n",
    "If it does contain an `extended_tweet` field the full text will be used.\n",
    "\n",
    "\n",
    "## TweetsODP\n",
    "Identifier | tweetsodp\n",
    "-----------|-------\n",
    "Importer   | [TweetsODPImporter](./tweetsodp.py)\n",
    "Source     | http://www.zubiaga.org/datasets/odptweets/\n",
    "Documents  | ~13000000\n",
    "Classes    | 15\n",
    "\n",
    "1. Gather data  \n",
    "   Download ODPtweets-Mar17-29.tar.bz2 and ODPtweets-Apr12-24.tar.bz2 to `data/raw/tweetsodp`.  \n",
    "2. Extract ids and classes  \n",
    "   Run python evaluation.py -s tweetsodp  \n",
    "   This will create ODPtweets-Apr12-24.txt and ODPtweets-Mar17-29.txt containing one tweet id per line.  \n",
    "3. Download Tweets  \n",
    "   - Download Hydrator from https://github.com/DocNow/hydrator  \n",
    "   - Open tweet id files from step 2 and set titles as ODPtweets-Apr12-24 and ODPtweets-Mar17-29 accordingly. \n",
    "   - This should create two json files, about 18GB each.  \n",
    "4. Cleanup (optional)  \n",
    "   You can delete the .txt files.  \n",
    "5. Compress (optional)\n",
    "   Zip the .json files as ODPtweets-Apr12-24.json.zip and ODPtweets-Mar17-29.json.zip to save storage space.\n",
    "\n",
    "\n",
    "## US Consumer Finance Complaints\n",
    "\n",
    "Identifier | classic4\n",
    "-----------|-------\n",
    "Importer   | [ComplaintsImporter](./complaints.py)\n",
    "Source     | https://www.kaggle.com/cfpb/us-consumer-finance-complaints\n",
    "Documents  | 66806 (715437)\n",
    "Classes    | 46 (90)\n",
    "\n",
    "Copy `consumer_complaints.csv` to `data/raw/complaints`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Markdown",
   "language": "markdown",
   "name": "markdown"
  },
  "language_info": {
   "codemirror_mode": "markdown",
   "file_extension": ".md",
   "mimetype": "text/markdown",
   "name": "Markdown",
   "pygments_lexer": "markdown"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
