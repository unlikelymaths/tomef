{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DefaultTokenizer\n",
    "<div style=\"position: absolute; right:0;top:0\"><a href=\"./tokenizer.ipynb\" style=\"text-decoration: none\"> <font size=\"5\">←</font></a>\n",
    "<a href=\"../evaluation.py.ipynb\" style=\"text-decoration: none\"> <font size=\"5\">↑</font></a></div>\n",
    "\n",
    "This module provides the `DefaultTokenizer` class that transforms the `text` of a document into `tokens`.\n",
    "It executes various functions that are controlled by the `token_info` settings.\n",
    "You can chose dataset, token version and a document to see the effect of various settings.\n",
    "The array `fixed_tokens` is used for all tokens that are no further processed.\n",
    "\n",
    "---\n",
    "## Setup and Settings\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __init__ import init_vars\n",
    "init_vars(vars(), ('info', {}), ('runvars', {}))\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "    \n",
    "from base import config, data\n",
    "from base.util import add_method\n",
    "from interface import nbprint, ProgressIterator\n",
    "\n",
    "import tokenizer.common\n",
    "import tokenizer.emoticons\n",
    "from tokenizer.util import iterate_tokens, TokenizerBase\n",
    "from tokenizer.widgets import token_picker, run_and_compare, show_comparison\n",
    "\n",
    "if RUN_SCRIPT: token_picker(info, runvars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tokenize Document\n",
    "---\n",
    "The following functions consitute the `DefaultTokenizer` class that transforms the raw text of a document into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS_OPTIONS = ['skip', 'keep', 'domain', 'replace', 'drop']\n",
    "EMOTES_OPTIONS = ['skip', 'keep', 'replace', 'drop']\n",
    "ALNUM_OPTIONS = ['skip', 'weak', 'apostrophe', 'strict']\n",
    "NUMBERS_OPTIONS = ['skip', 'decimal-on-one', 'all-on-one', 'drop']\n",
    "\n",
    "class DefaultTokenizer(TokenizerBase):\n",
    "    \n",
    "    def __init__(self, info):\n",
    "        super().__init__(info)\n",
    "        token_info = info.get('token_info', {})\n",
    "        \n",
    "        self.urls = token_info.get('urls', 'skip')\n",
    "        try:\n",
    "            self.urls_idx = URLS_OPTIONS.index(self.urls)\n",
    "        except ValueError:\n",
    "            raise config.ConfigException(('Invalid DefaultTokenizer configuration option urls \"{}\". '\n",
    "                                          'Valid options are \"{}\".').format(self.urls, '\", \"'.join(URLS_OPTIONS)))\n",
    "        \n",
    "        self.ascii_emotes   = token_info.get('ascii_emotes', 'skip')\n",
    "        try:\n",
    "            self.ascii_emotes_idx = EMOTES_OPTIONS.index(self.ascii_emotes)\n",
    "        except ValueError:\n",
    "            raise config.ConfigException(('Invalid DefaultTokenizer configuration option ascii_emotes \"{}\". '\n",
    "                                          'Valid options are \"{}\".').format(self.ascii_emotes, '\", \"'.join(EMOTES_OPTIONS)))\n",
    "        \n",
    "        self.unicode_emotes = token_info.get('unicode_emotes', 'skip')\n",
    "        try:\n",
    "            self.unicode_emotes_idx = EMOTES_OPTIONS.index(self.unicode_emotes)\n",
    "        except ValueError:\n",
    "            raise config.ConfigException(('Invalid DefaultTokenizer configuration option unicode_emotes \"{}\". '\n",
    "                                          'Valid options are \"{}\".').format(self.unicode_emotes, '\", \"'.join(EMOTES_OPTIONS)))\n",
    "        \n",
    "        self.alnum_only     = token_info.get('alnum_only', True)\n",
    "        try:\n",
    "            self.alnum_only_idx = ALNUM_OPTIONS.index(self.alnum_only)\n",
    "        except ValueError:\n",
    "            raise config.ConfigException(('Invalid DefaultTokenizer configuration option alnum_only \"{}\". '\n",
    "                                          'Valid options are \"{}\".').format(self.alnum_only, '\", \"'.join(ALNUM_OPTIONS)))\n",
    "        \n",
    "        self.numbers        = token_info.get('numbers', 'skip')\n",
    "        try:\n",
    "            self.numbers_idx = NUMBERS_OPTIONS.index(self.numbers)\n",
    "        except ValueError:\n",
    "            raise config.ConfigException(('Invalid DefaultTokenizer configuration option numbers \"{}\". '\n",
    "                                          'Valid options are \"{}\".').format(self.numbers, '\", \"'.join(NUMBERS_OPTIONS)))\n",
    "        \n",
    "        \n",
    "        self.lowercase      = token_info.get('lowercase', True)\n",
    "        self.numbers_split  = token_info.get('numbers_split', False)\n",
    "        self.ascii_only     = token_info.get('ascii_only', True)\n",
    "        \n",
    "if RUN_SCRIPT:\n",
    "    default_tokenizer = DefaultTokenizer(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare\n",
    "\n",
    "Splits the text at whitespace and initializes an empty list of fixed tokens. The `separator_token` is replaced with the `separator_token_replacement`, so that it can be used for saving the tokens as a string, i.e.\n",
    "```\n",
    "This:is:a:token:list\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultTokenizer)\n",
    "def init_tokenization(self, text):\n",
    "    self.fixed_tokens = []\n",
    "    self.text = text.replace(tokenizer.common.separator_token,tokenizer.common.separator_token_replacement)\n",
    "    self.tokens = text.split()\n",
    "\n",
    "if RUN_SCRIPT:\n",
    "    default_tokenizer.init_tokenization(runvars['document']['text'])\n",
    "    show_comparison(default_tokenizer.text, default_tokenizer.tokens, 'Text', 'Tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs\n",
    "Supports the following options for `urls`:\n",
    "- `skip`: this step will be skipped\n",
    "- `keep`: keeps every URL as it is, no further processing\n",
    "- `domain`: replaces every URL by its top and second level domain\n",
    "- `drop`: completely removes every URL from the text\n",
    "- `replace`: replaces every URL with the URL Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_urls_keep_fct(url_str):\n",
    "    return url_str\n",
    "def _process_urls_domain_fct(url_str):\n",
    "    for prefix in ['http://', 'https://']:\n",
    "        url_str = url_str.replace(prefix,'')\n",
    "    slash_index = url_str.find('/')\n",
    "    if slash_index > 0:\n",
    "        url_str = url_str[:slash_index]\n",
    "    if url_str.count('.') > 1:\n",
    "        url_str = url_str[url_str.rfind('.',0,url_str.rfind('.'))+1:]\n",
    "    return url_str\n",
    "def _process_urls_replace_fct(url_str):\n",
    "    return tokenizer.common.url_token\n",
    "def _process_urls_drop_fct(url_str):\n",
    "    pass\n",
    "_process_urls_fct_selector = [None,\n",
    "                              _process_urls_keep_fct, \n",
    "                              _process_urls_domain_fct, \n",
    "                              _process_urls_replace_fct, \n",
    "                              _process_urls_drop_fct]\n",
    "    \n",
    "@add_method(DefaultTokenizer)\n",
    "def process_urls_token(self, token):\n",
    "    if (token.startswith(\"http://\") or\n",
    "        token.startswith(\"https://\") or\n",
    "        token.startswith(\"www.\")):\n",
    "        url_str = self._process_urls_fct(token)\n",
    "        if url_str is not None:\n",
    "            self.fixed_tokens.append(url_str)\n",
    "        return None\n",
    "    return token\n",
    "\n",
    "@add_method(DefaultTokenizer)\n",
    "def process_urls(self):\n",
    "    self._process_urls_fct = _process_urls_fct_selector[self.urls_idx]\n",
    "    if self._process_urls_fct is not None:\n",
    "        iterate_tokens(self.tokens, self.process_urls_token)         \n",
    "    \n",
    "if RUN_SCRIPT:\n",
    "    run_and_compare(default_tokenizer, default_tokenizer.process_urls, 'tokens', 'fixed_tokens', 'Input', 'Fixed Tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASCII Emoticons\n",
    "\n",
    "Supports the following options for `ascii_emotes`:\n",
    "- `skip`: this step will be skipped\n",
    "- `keep`: keeps every ASCII emoticon as it is, no further processing\n",
    "- `drop`: ompletely removes every ASCII emoticon from the text\n",
    "- `replace`: replaces with the emote token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_emotes_keep_fct(emoticon):\n",
    "    return emoticon\n",
    "def _process_emotes_replace_fct(emoticon):\n",
    "    return tokenizer.common.emote_token\n",
    "def _process_emotes_drop_fct(emoticon):\n",
    "    pass\n",
    "_process_emotes_fct_selector = [None,\n",
    "                                _process_emotes_keep_fct,\n",
    "                                _process_emotes_replace_fct,\n",
    "                                _process_emotes_drop_fct]\n",
    "\n",
    "@add_method(DefaultTokenizer)\n",
    "def process_ascii_emotes_token(self, token):\n",
    "    for e, remainder in tokenizer.emoticons.western_dict.items():\n",
    "        parts = token.split(e, 1)\n",
    "        if len(parts) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            # Test if it is preceded by alphanumeric characters\n",
    "            pre = parts[0][-1:]\n",
    "            if pre.isalnum():\n",
    "                continue\n",
    "                \n",
    "            for r in remainder:\n",
    "                if not parts[1].startswith(r):\n",
    "                    continue\n",
    "                post = parts[1][len(r):]\n",
    "                \n",
    "                # Test if it is followed by alphanumeric characters\n",
    "                if post[:1].isalnum():\n",
    "                    continue\n",
    "                \n",
    "                emoticon = self._process_ascii_emotes_fct(e + r)\n",
    "                if emoticon is not None:\n",
    "                    self.fixed_tokens.append(emoticon)\n",
    "                remaining = []\n",
    "                if len(pre) >= 2:\n",
    "                    remaining += self.process_ascii_emotes_token(pre)\n",
    "                else:\n",
    "                    remaining += pre\n",
    "                if len(post) >= 2:\n",
    "                    remaining += self.process_ascii_emotes_token(post)\n",
    "                else:\n",
    "                    remaining += post\n",
    "                return [s for s in parts if len(s) > 0]\n",
    "    return token\n",
    "\n",
    "@add_method(DefaultTokenizer)\n",
    "def process_ascii_emotes(self):\n",
    "    self._process_ascii_emotes_fct = _process_emotes_fct_selector[self.ascii_emotes_idx]\n",
    "    if self._process_ascii_emotes_fct is not None:\n",
    "        iterate_tokens(self.tokens, self.process_ascii_emotes_token)    \n",
    "            \n",
    "if RUN_SCRIPT:\n",
    "    run_and_compare(default_tokenizer, default_tokenizer.process_ascii_emotes, 'tokens', 'fixed_tokens', 'Input', 'Fixed Tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode Emoticons\n",
    "\n",
    "Supports the following options for `unicode_emotes`:\n",
    "- `skip`: this step will be skipped\n",
    "- `keep`: keeps every ASCII emoticon as it is, no further processing\n",
    "- `drop`: ompletely removes every ASCII emoticon from the text\n",
    "- `replace`: replaces with the emote token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultTokenizer)\n",
    "def process_unicode_emotes_token(self, token):\n",
    "    tokens = []\n",
    "    new_token = ''\n",
    "    for c in token:\n",
    "        if c in emoji.UNICODE_EMOJI:\n",
    "            emoticon = self._process_ascii_emotes_fct(c)\n",
    "            if emoticon is not None:\n",
    "                self.fixed_tokens.append(emoticon)\n",
    "            if len(new_token) > 0:\n",
    "                tokens.append(new_token)\n",
    "                new_token = ''\n",
    "        else:\n",
    "            new_token += c\n",
    "    if len(tokens) == 0:\n",
    "        return new_token\n",
    "    return tokens + [new_token]\n",
    "    \n",
    "@add_method(DefaultTokenizer)\n",
    "def process_unicode_emoticons(self):\n",
    "    self._process_ascii_emotes_fct = _process_emotes_fct_selector[self.ascii_emotes_idx]\n",
    "    if self._process_ascii_emotes_fct is not None:\n",
    "        iterate_tokens(self.tokens, self.process_unicode_emotes_token) \n",
    "        \n",
    "if RUN_SCRIPT:\n",
    "    run_and_compare(default_tokenizer, default_tokenizer.process_unicode_emoticons, 'tokens', 'fixed_tokens', 'Input', 'Fixed Tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Numbers\n",
    "\n",
    "This step splits words that consist of letters, special characters and numbers into distinct words.\n",
    "`,` and `.` are allowed to occur within numbers and do not lead to splitting up the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dezimal_re = re.compile('([0-9]+(?:[,.]+[0-9,.]+)*)')\n",
    "\n",
    "@add_method(DefaultTokenizer)\n",
    "def split_numbers_token(self, token):\n",
    "    return [s for s in dezimal_re.split(token) if len(s) > 0]\n",
    "\n",
    "@add_method(DefaultTokenizer)\n",
    "def split_numbers(self):\n",
    "    if self.numbers_split:\n",
    "        iterate_tokens(self.tokens, self.split_numbers_token) \n",
    "        \n",
    "if RUN_SCRIPT:\n",
    "    run_and_compare(default_tokenizer, default_tokenizer.split_numbers, 'tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase\n",
    "\n",
    "All letters are lowercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultTokenizer)\n",
    "def process_lowercase_token(self, token):\n",
    "    return token.lower()\n",
    "\n",
    "@add_method(DefaultTokenizer)\n",
    "def process_lowercase(self):\n",
    "    if self.lowercase:\n",
    "        iterate_tokens(self.tokens, self.process_lowercase_token)\n",
    "        \n",
    "if RUN_SCRIPT:\n",
    "    run_and_compare(default_tokenizer, default_tokenizer.process_lowercase, 'tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove non-ascii characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultTokenizer)\n",
    "def remove_nonascii_token(self, token):\n",
    "    return token.encode('ascii',errors='ignore').decode()\n",
    "\n",
    "@add_method(DefaultTokenizer)\n",
    "def remove_nonascii(self):\n",
    "    if self.ascii_only:\n",
    "        iterate_tokens(self.tokens, self.remove_nonascii_token)\n",
    "        \n",
    "if RUN_SCRIPT:\n",
    "    run_and_compare(default_tokenizer, default_tokenizer.remove_nonascii, 'tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Non-alphanumeric characters\n",
    "\n",
    "Removes non alphanumeric characters. Possible options for `alnum_only` are:\n",
    "- `skip`: Retains all characters\n",
    "- `weak`: Retains `'`, `@`, `#`, and `_`\n",
    "- `apostrophe`: Retains `'`\n",
    "- `strict`: Removed all characters except `a-z`, `A-Z` and `0-9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_remove_nonalnum_re_selector = [\n",
    "    None,\n",
    "    re.compile('[^a-zA-Z0-9\\'@#_]'),\n",
    "    re.compile('[^a-zA-Z0-9\\']'),\n",
    "    re.compile('[^a-zA-Z0-9]')]\n",
    "\n",
    "@add_method(DefaultTokenizer)\n",
    "def remove_nonalnum_token(self, token):\n",
    "    return [s \n",
    "            for s in _remove_nonalnum_re_selector[self.alnum_only_idx].split(token) \n",
    "            if len(s) > 0]\n",
    "\n",
    "@add_method(DefaultTokenizer)\n",
    "def remove_nonalnum(self):\n",
    "    if self.alnum_only:\n",
    "        iterate_tokens(self.tokens, self.remove_nonalnum_token)\n",
    "    \n",
    "if RUN_SCRIPT:\n",
    "    run_and_compare(default_tokenizer, default_tokenizer.remove_nonalnum, 'tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Numbers\n",
    "\n",
    "Replace numbers by number tokens. Supports the following options for `numbers`:\n",
    "- `skip`: \n",
    "Either replace each single digit by a token or replace the whole number (possibly including `.` and `,`) by a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_re_numbers_decimal  = re.compile(\"[0-9]\")\n",
    "_re_numbers_complete = re.compile(\"([0-9][0-9\\.,]*)|([0-9\\.,]*[0-9])\")\n",
    "_replace_numbers_re_selector = [None,\n",
    "    _re_numbers_decimal,\n",
    "    _re_numbers_complete,\n",
    "    _re_numbers_complete]\n",
    "_replace_numbers_sub_selector = [None,\n",
    "    tokenizer.common.number_token,\n",
    "    tokenizer.common.number_token,\n",
    "    '']\n",
    "\n",
    "@add_method(DefaultTokenizer)\n",
    "def replace_numbers_token(self, token):\n",
    "    token, count = self._replace_numbers_re.subn(self._replace_numbers_sub, token)\n",
    "    if count > 0:\n",
    "        self.fixed_tokens.append(token)\n",
    "        return None\n",
    "    return token\n",
    "\n",
    "@add_method(DefaultTokenizer)\n",
    "def replace_numbers(self):\n",
    "    if self.numbers_idx > 0:\n",
    "        self._replace_numbers_re= _replace_numbers_re_selector[self.numbers_idx]\n",
    "        self._replace_numbers_sub = _replace_numbers_sub_selector[self.numbers_idx]\n",
    "        iterate_tokens(self.tokens, self.replace_numbers_token)\n",
    "        \n",
    "if RUN_SCRIPT: \n",
    "    run_and_compare(default_tokenizer, default_tokenizer.replace_numbers, 'tokens', 'fixed_tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Complete function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultTokenizer)\n",
    "def tokenize(self, text, *args):\n",
    "    self.init_tokenization(text)\n",
    "    self.process_urls()\n",
    "    self.process_ascii_emotes()\n",
    "    self.process_unicode_emoticons()\n",
    "    self.split_numbers()\n",
    "    self.process_lowercase()\n",
    "    self.remove_nonascii()\n",
    "    self.remove_nonalnum()\n",
    "    self.replace_numbers()\n",
    "    self.tokens = self.tokens + self.fixed_tokens\n",
    "    return self.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SCRIPT:\n",
    "    default_tokenizer = DefaultTokenizer(info)\n",
    "    default_tokenizer.tokenize(runvars['document']['text'])\n",
    "    show_comparison(default_tokenizer.text, default_tokenizer.tokens, 'Text', 'Tokens')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
