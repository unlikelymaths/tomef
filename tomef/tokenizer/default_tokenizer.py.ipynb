{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DefaultTokenizer\n",
    "<div style=\"position: absolute; right:0;top:0\"><a href=\"./tokenizer.ipynb\" style=\"text-decoration: none\"> <font size=\"5\">←</font></a>\n",
    "<a href=\"../evaluation.py.ipynb\" style=\"text-decoration: none\"> <font size=\"5\">↑</font></a></div>\n",
    "\n",
    "This module provides the `DefaultTokenizer` class that transforms the `text` of a document into `tokens`.\n",
    "It executes various functions that are controlled by the `token_info` settings.\n",
    "You can chose dataset, token version and a document to see the effect of various settings.\n",
    "The array `fixed_tokens` is used for all tokens that are no further processed.\n",
    "\n",
    "---\n",
    "## Setup and Settings\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __init__ import init_vars\n",
    "init_vars(vars(), ('info', {}), ('runvars', {}))\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "    \n",
    "import data\n",
    "import config\n",
    "from base import nbprint\n",
    "from util import ProgressIterator\n",
    "\n",
    "import tokenizer.common\n",
    "import tokenizer.emoticons\n",
    "from tokenizer.token_util import iterate_tokens, TokenizerBase\n",
    "from tokenizer.widgets import token_picker, run_and_compare\n",
    "\n",
    "if RUN_SCRIPT: token_picker(info, runvars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tokenize Document\n",
    "---\n",
    "The following functions consitute the `DefaultTokenizer` class that transforms the raw text of a document into tokens. The default options are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_options = {\n",
    "    'urls': 'skip',\n",
    "    'ascii_emotes': 'skip',\n",
    "    'unicode_emotes': 'skip',\n",
    "    'numbers': 'drop',\n",
    "    'lowercase': True,\n",
    "    'numbers_split': False,\n",
    "    'alnum_only': True,\n",
    "    'ascii_only': True,\n",
    "}\n",
    "def get_option(token_info, option_key):\n",
    "    return token_info.get(option_key, default_options[option_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs\n",
    "Supports the following options for `urls`:\n",
    "- `skip`: this step will be skipped\n",
    "- `keep`: keeps every URL as it is, no further processing\n",
    "- `domain`: replaces every URL by its top and second level domain\n",
    "- `drop`: completely removes every URL from the text\n",
    "- `replace`: replaces every URL with the URL Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_drop_fct(runvars, url_str):\n",
    "    pass\n",
    "def url_domain_fct(runvars, url_str):\n",
    "    for prefix in ['http://', 'https://']:\n",
    "        url_str = url_str.replace(prefix,'')\n",
    "    slash_index = url_str.find('/')\n",
    "    if slash_index > 0:\n",
    "        url_str = url_str[:slash_index]\n",
    "    if url_str.count('.') > 1:\n",
    "        url_str = url_str[url_str.rfind('.',0,url_str.rfind('.'))+1:]\n",
    "    runvars['fixed_tokens'].append(url_str)\n",
    "def url_replace_fct(runvars, url_str):\n",
    "    runvars['fixed_tokens'].append(tokenizer.common.url_token) \n",
    "def url_keep_fct(runvars, url_str):\n",
    "    runvars['fixed_tokens'].append(url_str) \n",
    "def url_fct_selector(option):\n",
    "    if option == \"drop\":\n",
    "        return url_drop_fct\n",
    "    elif option == \"domain\":\n",
    "        return url_domain_fct\n",
    "    elif option == \"replace\":\n",
    "        return url_replace_fct\n",
    "    elif option == \"keep\":\n",
    "        return url_keep_fct\n",
    "    raise config.ConfigException('token_info setting \"{}\" for urls not defined.'.format(option))\n",
    "    \n",
    "def process_urls(info, runvars):\n",
    "    option = get_option(info['token_info'],'urls')\n",
    "    runvars['fixed_tokens'] = []\n",
    "    if option == 'skip':\n",
    "        runvars['text'] = runvars['document']['text']\n",
    "    else:\n",
    "        url_fct = url_fct_selector(option)\n",
    "        words = runvars['document']['text'].split()\n",
    "        new_words = []\n",
    "        runvars['fixed_tokens'] = []\n",
    "        for word in words:\n",
    "            if (word.startswith(\"http://\") or\n",
    "                word.startswith(\"https://\") or\n",
    "                word.startswith(\"www.\")):\n",
    "                url_fct(runvars, word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        runvars['text'] = ' '.join(new_words)\n",
    "    \n",
    "if RUN_SCRIPT: run_and_compare(info, runvars, process_urls, ['document','text'], 'fixed_tokens', 'Input', 'Fixed Tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASCII Emoticons\n",
    "\n",
    "Supports the following options for `ascii_emotes`:\n",
    "- `skip`: this step will be skipped\n",
    "- `keep`: keeps every ASCII emoticon as it is, no further processing\n",
    "- `drop`: ompletely removes every ASCII emoticon from the text\n",
    "- `replace`: replaces with the emote token\n",
    "\n",
    "**TODO**: Prevent it from removing parts of words, e.g. `xp` from `experiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoticon_drop_fct(runvars,emoticon,count=1):\n",
    "    pass\n",
    "def emoticon_keep_fct(runvars,emoticon,count=1):\n",
    "    runvars['fixed_tokens'] += [emoticon,] * count\n",
    "def emoticon_replace_fct(runvars,emoticon,count=1):\n",
    "    runvars['fixed_tokens'] += [tokenizer.common.emote_token,] * count\n",
    "def emoticon_fct_selector(option):\n",
    "    if option == 'keep':\n",
    "        return emoticon_keep_fct\n",
    "    elif option == 'drop':\n",
    "        return emoticon_drop_fct\n",
    "    elif option == 'replace':\n",
    "        return emoticon_replace_fct\n",
    "    raise config.ConfigException('token_info setting \"{}\" for emoticons not defined.'.format(option))\n",
    "\n",
    "\n",
    "def process_ascii_emoticons(info, runvars):\n",
    "    option = get_option(info['token_info'],'ascii_emotes')\n",
    "    if option != 'skip':\n",
    "        emoticon_fct = emoticon_fct_selector(option)\n",
    "        text = runvars['text']\n",
    "        for e in tokenizer.emoticons.western:\n",
    "            ecount = text.count(e)\n",
    "            if ecount > 0:\n",
    "                emoticon_fct(runvars,e,ecount)\n",
    "                text = text.replace(e, ' ')\n",
    "        runvars['text'] = text\n",
    "            \n",
    "if RUN_SCRIPT: run_and_compare(info, runvars, process_ascii_emoticons, 'text', 'fixed_tokens', 'Input', 'Fixed Tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode Emoticons\n",
    "\n",
    "Supports the following options for `unicode_emotes`:\n",
    "- `skip`: this step will be skipped\n",
    "- `keep`: keeps every ASCII emoticon as it is, no further processing\n",
    "- `drop`: ompletely removes every ASCII emoticon from the text\n",
    "- `replace`: replaces with the emote token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_unicode_emoticons(info, runvars):\n",
    "    option = get_option(info['token_info'],'unicode_emotes')\n",
    "    if option != 'skip':\n",
    "        emoticon_fct = emoticon_fct_selector(option)\n",
    "        text = runvars['text']\n",
    "        new_text = ''\n",
    "        for c in text:\n",
    "            if c in emoji.UNICODE_EMOJI:\n",
    "                emoticon_fct(runvars,c)\n",
    "                new_text += ' '\n",
    "            else:\n",
    "                new_text += c\n",
    "        runvars['text'] = new_text\n",
    "            \n",
    "if RUN_SCRIPT: run_and_compare(info, runvars, process_unicode_emoticons, 'text', 'fixed_tokens', 'Input', 'Fixed Tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split\n",
    "\n",
    "The text is split into tokens. The `separator_token` is replaced with the `separator_token_replacement`, so that it can be used for saving the tokens as a string, i.e.\n",
    "```\n",
    "This;is;a;token;list\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(info, runvars):\n",
    "    text = runvars['text']\n",
    "    text = text.replace(tokenizer.common.separator_token,tokenizer.common.separator_token_replacement)\n",
    "    runvars['tokens'] = text.split()\n",
    "if RUN_SCRIPT: run_and_compare(info, runvars, split, 'text', 'tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase (Optional)\n",
    "\n",
    "All letters are lowercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_word(word):\n",
    "    return word.lower()\n",
    "\n",
    "def lowercase(info, runvars):\n",
    "    if get_option(info['token_info'],'lowercase'):\n",
    "        iterate_tokens(runvars['tokens'], lowercase_word)\n",
    "        \n",
    "if RUN_SCRIPT: run_and_compare(info, runvars, lowercase, 'tokens', 'tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Numbers (Optional)\n",
    "\n",
    "This step splits words that consist of letters and numbers into distinct words.\n",
    "`ignore_in_decimal` are characters that are allowed to occur within numbers and do not lead to splitting up the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_numbers_word(word):\n",
    "    words = []\n",
    "    current_word = word[0]\n",
    "    current_word_isdecimal = current_word.isdecimal()\n",
    "    ignore_in_decimal = [\",\",\".\"]\n",
    "    ignore_count = 0 # counts how many consecutive ignore_in_decimal characters have appeared\n",
    "    for char in word[1:]:\n",
    "        if char.isdecimal() != current_word_isdecimal:\n",
    "            if current_word_isdecimal and char in ignore_in_decimal and ignore_count == 0:\n",
    "                ignore_count += 1\n",
    "                current_word += char\n",
    "            else:\n",
    "                if ignore_count:\n",
    "                    current_word = current_word[:-ignore_count]\n",
    "                    ignore_count = 0\n",
    "                words.append(current_word)\n",
    "                current_word = char\n",
    "                current_word_isdecimal = current_word.isdecimal()\n",
    "        else:\n",
    "            ignore_count = 0\n",
    "            current_word += char\n",
    "    \n",
    "    if ignore_count:\n",
    "        current_word = current_word[:-ignore_count]\n",
    "    words.append(current_word)\n",
    "    return words\n",
    "\n",
    "def split_numbers(info, runvars):\n",
    "    if get_option(info['token_info'],'numbers_split'):\n",
    "        iterate_tokens(runvars['tokens'], split_numbers_word)\n",
    "        \n",
    "if RUN_SCRIPT: run_and_compare(info, runvars, split_numbers, 'tokens', 'tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Non-alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_number_only_single = re.compile(\"^(\\d+)[\\.,](\\d+)$\")\n",
    "re_word_single_apostrophe = re.compile(\"^[^']+'[^']{1,3}$\")\n",
    "\n",
    "def remove_nonalnum_word(word):\n",
    "    if word.isalnum():\n",
    "        return word\n",
    "    \n",
    "    if re_number_only_single.match(word):\n",
    "        return word\n",
    "    elif re_word_single_apostrophe.match(word):\n",
    "        parts = word.split(\"'\")\n",
    "        part_left = ''.join([char for char in parts[0] if char.isalnum()])\n",
    "        part_right = ''.join([char for char in parts[1] if char.isalnum()])\n",
    "        return \"'\".join([part_left, part_right])\n",
    "        \n",
    "    new_word = [char for char in word if char.isalnum()]\n",
    "    return ''.join(new_word)\n",
    "\n",
    "def remove_nonalnum(info, runvars):\n",
    "    if get_option(info['token_info'],'alnum_only'):\n",
    "        iterate_tokens(runvars['tokens'], remove_nonalnum_word)\n",
    "    \n",
    "if RUN_SCRIPT: run_and_compare(info, runvars, remove_nonalnum, 'tokens', 'tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove non-ascii characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonascii_word(word):\n",
    "    return word.encode('ascii',errors='ignore').decode()\n",
    "\n",
    "def remove_nonascii(info, runvars):\n",
    "    if get_option(info['token_info'],'ascii_only'):\n",
    "        iterate_tokens(runvars['tokens'], remove_nonascii_word)\n",
    "        \n",
    "if RUN_SCRIPT: run_and_compare(info, runvars, remove_nonascii, 'tokens', 'tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_number_single = re.compile(\"[0-9]\")\n",
    "def _replace_numbers(word):\n",
    "    return re_number_single.sub(tokenizer.common.number_token, word)\n",
    "\n",
    "\n",
    "re_number_complete = re.compile(\"([0-9][0-9\\.,]*)|([0-9\\.,]*[0-9])\")\n",
    "def _replace_numbers_single(word):\n",
    "    return re_number_complete.sub(tokenizer.common.number_token, word)\n",
    "\n",
    "re_number_only_single = re.compile(\"^(\\d+)[\\.,](\\d+)$\")\n",
    "def _replace_numbers_drop(word):\n",
    "    return re_number_complete.sub(\"\", word)\n",
    "\n",
    "replace_numbers_selector = {\n",
    "    'replace': _replace_numbers,\n",
    "    'replace_single': _replace_numbers_single,\n",
    "    'drop': _replace_numbers_drop\n",
    "}\n",
    "\n",
    "def replace_numbers(info, runvars):\n",
    "    option = get_option(info['token_info'],'numbers')\n",
    "    if option == \"keep\":\n",
    "        pass\n",
    "    else:\n",
    "        iterate_tokens(runvars['tokens'], replace_numbers_selector[option])\n",
    "        \n",
    "if RUN_SCRIPT: run_and_compare(info, runvars, replace_numbers, 'tokens', 'tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build DefaultTokenizer class\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultTokenizer(TokenizerBase):\n",
    "    \n",
    "    def tokenize(self, text, *args):\n",
    "        runvars = {'document': {'text': text}}\n",
    "        \n",
    "        process_urls(self.info, runvars)\n",
    "        process_ascii_emoticons(self.info, runvars)\n",
    "        process_unicode_emoticons(self.info, runvars)\n",
    "        split(self.info, runvars)\n",
    "        lowercase(self.info, runvars)\n",
    "        split_numbers(self.info, runvars)\n",
    "        remove_nonalnum(self.info, runvars)\n",
    "        remove_nonascii(self.info, runvars)\n",
    "        replace_numbers(self.info, runvars)\n",
    "        \n",
    "        return runvars['tokens'] + runvars['fixed_tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_default_tokenizer(info, runvars):\n",
    "    token = DefaultTokenizer(info)\n",
    "    runvars['tokens'] = token.tokenize(runvars['document']['text'])\n",
    "\n",
    "if RUN_SCRIPT: run_and_compare(info, runvars, execute_default_tokenizer, ['document','text'], 'tokens')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
