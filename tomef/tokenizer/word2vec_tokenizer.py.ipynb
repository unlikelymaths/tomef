{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vecTokenizer\n",
    "<div style=\"position: absolute; right:0;top:0\"><a href=\"./tokenizer.ipynb\" style=\"text-decoration: none\"> <font size=\"5\">←</font></a>\n",
    "<a href=\"../evaluation.py.ipynb\" style=\"text-decoration: none\"> <font size=\"5\">↑</font></a></div>\n",
    "\n",
    "This module provides the `W2VTokenizer` class that transforms the `text` of a document into `tokens`.\n",
    "It keeps only those tokens that appear in the vocabulary of the corresponding embedding model,\n",
    "but tries to combine tokens into phrases if they appear in the model.\n",
    "\n",
    "---\n",
    "## Setup and Settings\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __init__ import init_vars\n",
    "init_vars(vars(), ('info', {}), ('runvars', {}))\n",
    "\n",
    "import re\n",
    "    \n",
    "from base import data, config\n",
    "from base.util import add_method\n",
    "from interface import nbprint, nbbox, ProgressIterator\n",
    "\n",
    "from embedding.main import get_model\n",
    "\n",
    "import tokenizer.common\n",
    "from tokenizer.util import TokenizerBase\n",
    "from tokenizer.default_tokenizer import DefaultTokenizer\n",
    "from tokenizer.widgets import token_picker, run_and_compare, show_comparison\n",
    "\n",
    "if RUN_SCRIPT: token_picker(info, runvars, 'C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tokenize Document\n",
    "---\n",
    "The following functions consitute the `W2VTokenizer` class that transforms the raw text of a document into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2VTokenizer(TokenizerBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.embedding_model = get_model(self.info)\n",
    "        self.filter = self.embedding_model.filter.filter\n",
    "\n",
    "if RUN_SCRIPT:\n",
    "    with nbbox():\n",
    "        w2v_tokenizer = W2VTokenizer(info)\n",
    "        w2v_tokenizer.text = runvars['document']['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Text\n",
    "\n",
    "This step lowercases all characters and replaces the following:\n",
    "- `separator_token` by `separator_token_replacement`\n",
    "- all whitespaces by a single whitespace\n",
    "- `#` by nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_re_whitespace = re.compile('[\\s]+', re.UNICODE)\n",
    "_re_url = re.compile('(http://[^\\s]+)|(https://[^\\s]+)|(www\\.[^\\s]+)')\n",
    "\n",
    "@add_method(W2VTokenizer)\n",
    "def prepare(self):\n",
    "    self.text = self.text.lower()\n",
    "    self.text = self.text.replace(tokenizer.common.separator_token,tokenizer.common.separator_token_replacement)\n",
    "    self.text = self.text.replace('#', '')\n",
    "    self.text, count = _re_url.subn(' ', self.text)\n",
    "    self.text, count = _re_whitespace.subn(' ', self.text)\n",
    "    \n",
    "if RUN_SCRIPT:\n",
    "    run_and_compare(w2v_tokenizer, w2v_tokenizer.prepare, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace numbers\n",
    "\n",
    "All numbers are replaced by `#`. This include all numbers in the Unicode 'Number, Decimal Digit' category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_re_decimal = re.compile('\\d', re.UNICODE)\n",
    "\n",
    "@add_method(W2VTokenizer)\n",
    "def replace_numbers(self):\n",
    "    self.text, count = _re_decimal.subn('#', self.text)\n",
    "    \n",
    "if RUN_SCRIPT:\n",
    "    run_and_compare(w2v_tokenizer, w2v_tokenizer.replace_numbers, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split at breaking characters\n",
    "\n",
    "This step splits the string into substrings $s_i$ at all sequences of non alphanumeric characters (`\\w`), whitespace (`\\s`), or apostrophes (`\\'`). Later, the algorithm will only try to combine tokens from each $s_i$ separately into phrases, but not tokens from different substrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_re_breaking = re.compile('[^\\w\\s\\'\\’#]+', re.UNICODE)\n",
    "\n",
    "@add_method(W2VTokenizer)\n",
    "def split_text(self):\n",
    "    self.subtexts = _re_breaking.split(self.text)\n",
    "    \n",
    "if RUN_SCRIPT:\n",
    "    run_and_compare(w2v_tokenizer, w2v_tokenizer.split_text, 'text', 'subtexts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split at nonbreaking characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(W2VTokenizer)\n",
    "def split_subtexts(self):\n",
    "    self.tokenlists = [subtext.split()\n",
    "                      for subtext in self.subtexts\n",
    "                      if len(subtext) > 0]\n",
    "    \n",
    "if RUN_SCRIPT:\n",
    "    run_and_compare(w2v_tokenizer, w2v_tokenizer.split_subtexts, 'subtexts', 'tokenlists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(W2VTokenizer)\n",
    "def build_tokens(self):\n",
    "    self.tokens = []\n",
    "    for tokenlist in self.tokenlists:\n",
    "        self.tokens = self.tokens + self.filter(tokenlist)\n",
    "    \n",
    "if RUN_SCRIPT:\n",
    "    run_and_compare(w2v_tokenizer, w2v_tokenizer.build_tokens, 'tokenlists', 'tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Complete function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(W2VTokenizer)\n",
    "def tokenize(self, text, *args):\n",
    "    self.text = text\n",
    "    self.prepare()\n",
    "    self.replace_numbers()\n",
    "    self.split_text()\n",
    "    self.split_subtexts()\n",
    "    self.build_tokens()\n",
    "    return self.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SCRIPT:\n",
    "    w2v_tokenizer.tokenize(runvars['document']['text'])\n",
    "    show_comparison(runvars['document']['text'], w2v_tokenizer.tokens, 'Text', 'Tokens')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
