{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "<div style=\"position: absolute; right:0;top:0\"><a href=\"../evaluation.py.ipynb\" style=\"text-decoration: none\"> <font size=\"5\">â†‘</font></a></div>\n",
    "\n",
    "This module computes various metrics to evaluate the topic models.\n",
    "\n",
    "--- \n",
    "\n",
    "#### [Metric Viewer](./metric_viewer.app.ipynb)\n",
    "\n",
    "Show all metric results.\n",
    "\n",
    "---\n",
    "\n",
    "## General\n",
    "\n",
    "The metrics module is somewhat different from the previous modules in that all outputs are written to a single file and it will only evaluate available results, i.e.\n",
    "it will not run any previous module automatically.\n",
    "Similar to previous modules it will not compute a metric if the entry already exists in the file.\n",
    "\n",
    "### Helper\n",
    "\n",
    "- `load_ground_truth_classes`: loads ground truth classes from file\n",
    "\n",
    "## Model Metrics\n",
    "\n",
    "Those metrics work solely on the results of the Model module.\n",
    "They are all evaluated with respect to ground truth classification information and can not be applied if those labels are missing.\n",
    "Each Model has to return at least an H matrix that is a nonnegative topics-by-documents matrix. \n",
    "Each entry represents the affinity of the corresponding topic and document. \n",
    "Alternatively a model may return a class array of length equal to the number of documents where each entry is an integer $k \\in\\{1,...,\\text{num_topics}\\}$.\n",
    "Note that the ids of the topics do not neccessarily correspond to ground truth labels.\n",
    "An H matrix can be converted into a class array by finding the maximum index of each column. \n",
    "\n",
    "### Clustering\n",
    "\n",
    "The following metrics can be used to evaluate class arrays:\n",
    "\n",
    "- [Normalized Mutual Information (NMI)](./clustering.ipynb#NMI)\n",
    "- [Adjusted Rand index (ARI)](./clustering.ipynb#ARI)\n",
    "\n",
    "### Classification \n",
    "\n",
    "If the model returns an H matrix it is possible to train a classifier on the matrix and evaluate its classification performance using the following metrics.\n",
    "5 fold cross validation is applied.\n",
    "\n",
    "#### Classifiers\n",
    "- SVM\n",
    "\n",
    "#### Metrics\n",
    "- micro averaged F1 score\n",
    "\n",
    "\n",
    "\n",
    "## Distiller Metrics\n",
    "\n",
    "`TODO`\n",
    "\n",
    "## Details\n",
    "\n",
    "**Clustering**  \n",
    "Each clustering metric is defined in config.metrics['clustering'] as\n",
    "```json\n",
    "\"identifier\": {\n",
    "        \"name\": STRING,\n",
    "        \"run\": BOOLEAN,\n",
    "        \"filename\": STRING,\n",
    "        \"function\": STRING\n",
    "      }\n",
    "```\n",
    "The function should have the following signature\n",
    "```python\n",
    "def my_metric(labels_true, labels_pred):\n",
    "    # compute metric\n",
    "    return metric\n",
    "```\n",
    "which is analogue to scikit learn metrics. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
