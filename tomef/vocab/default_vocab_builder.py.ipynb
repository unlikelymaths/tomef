{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab Builder\n",
    "<div style=\"position: absolute; right:0;top:0\"><a href=\"./vocab.ipynb\" style=\"text-decoration: none\"> <font size=\"5\">←</font></a>\n",
    "<a href=\"../evaluation.py.ipynb\" style=\"text-decoration: none\"> <font size=\"5\">↑</font></a></div>\n",
    "\n",
    "This module provides the `DefaultVocabBuilder` class that transforms the `tokens` of all documents into a `vocab`.\n",
    "First it counts all occurences of tokens in the dataset and the number of documents they appear in (`count_tokens()`).\n",
    "Subsequently, it executes various functions that are controlled by the `vocab_info` settings.\n",
    "You can chose dataset, token version and vector version to see the effect of various settings.\n",
    "\n",
    "---\n",
    "## Setup and Settings\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __init__ import init_vars\n",
    "init_vars(vars(), ('info', {}))\n",
    "\n",
    "from operator import attrgetter\n",
    "            \n",
    "from base import config, data\n",
    "from base.util import add_method\n",
    "from interface import nbprint, nbbox, ProgressIterator\n",
    "\n",
    "from tokenizer.common import split_tokens\n",
    "\n",
    "from vocab.widgets import vocab_picker, show_tokens\n",
    "from vocab.util import VocabItem,VocabBuilder\n",
    "from vocab.stopwords import nltk_stopwords\n",
    "\n",
    "if RUN_SCRIPT: vocab_picker(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Vocab Builder\n",
    "---\n",
    "\n",
    "The following functions consitute the `DefaultVocabBuilder` class that produces a vocabulary by counting all tokens in a dataset and reducing the set of tokens according to various filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultVocabBuilder(VocabBuilder):\n",
    "    \n",
    "    def __init__(self, info):\n",
    "        super().__init__(info)\n",
    "        vocab_info = info.get('vocab_info', {})\n",
    "        \n",
    "        self.min_docs = vocab_info.get('min_docs', False)\n",
    "        self.max_docs = vocab_info.get('max_docs', False)\n",
    "        self.min_count = vocab_info.get('min_count', False)\n",
    "        self.max_count = vocab_info.get('max_count', False)\n",
    "        self.min_word_length = vocab_info.get('min_word_length', False)\n",
    "        self.max_word_length = vocab_info.get('max_word_length', False)\n",
    "        self.stopwords = vocab_info.get('stopwords', False)\n",
    "        self.max_tokens = vocab_info.get('max_tokens', False)\n",
    "\n",
    "        #self.urls = token_info.get('urls', 'skip')\n",
    "        #try:\n",
    "        #    self.urls_idx = URLS_OPTIONS.index(self.urls)\n",
    "        #except ValueError:\n",
    "        #    raise config.ConfigException(('Invalid DefaultTokenizer configuration option urls \"{}\". '\n",
    "        #                                  'Valid options are \"{}\".').format(self.urls, '\", \"'.join(URLS_OPTIONS)))\n",
    "        \n",
    "if RUN_SCRIPT:\n",
    "    default_vocab_builder = DefaultVocabBuilder(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative to absolute counts\n",
    "\n",
    "As some settings can be given in relative terms (e.g. `min_docs = 0.01` to only allow tokens that appear in at least 1% of the documents) this method transforms any number `<1` to an absolute count value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultVocabBuilder)\n",
    "def to_abs(self, count):\n",
    "    if count <= 0:\n",
    "        return 0\n",
    "    elif count < 1:\n",
    "        return int(count * self.num_docs)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Count tokens\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `def count_tokens()`  \n",
    "Iterates over all tokens and accumulates counts in `rawcounts` dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultVocabBuilder)\n",
    "def count_tokens(self):\n",
    "    self.rawcounts = {} \n",
    "    self.num_docs = 0\n",
    "    with data.tokenized_document_reader(self.info) as documents:   \n",
    "        for document in ProgressIterator(documents, 'Counting Tokens'):\n",
    "            self.num_docs += 1\n",
    "            tokens = split_tokens(document['tokens'])\n",
    "            for token in tokens:\n",
    "                try:\n",
    "                    self.rawcounts[token].increase_total()\n",
    "                except KeyError:\n",
    "                    self.rawcounts[token] = VocabItem(token, total=1)\n",
    "            for token in set(tokens):\n",
    "                self.rawcounts[token].increase_document() \n",
    "    self.num_tokens = len(self.rawcounts)\n",
    "if RUN_SCRIPT:\n",
    "    with nbbox():\n",
    "        default_vocab_builder.count_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `def sort_counts()`  \n",
    "Turn `rawcounts` dict into list and sort tokens by number of total occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultVocabBuilder)\n",
    "def sort_counts(self):\n",
    "    self.counts = sorted(self.rawcounts.values(), \n",
    "        key=attrgetter('total'),\n",
    "        reverse=True)\n",
    "if RUN_SCRIPT:\n",
    "    default_vocab_builder.sort_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the tokens with the highest total counts and some random ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SCRIPT:\n",
    "    show_tokens(default_vocab_builder.counts,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Filter Tokens\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `def filter_min_docs()`\n",
    "Remove tokens occuring in less than `min_docs` documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultVocabBuilder)\n",
    "def filter_min_docs(self):\n",
    "    if self.min_docs:\n",
    "        min_docs = self.to_abs(self.min_docs)\n",
    "        self.counts = [vocab_item \n",
    "                       for vocab_item in self.counts\n",
    "                       if vocab_item.document >= min_docs]\n",
    "        num_removed = self.num_tokens - len(self.counts)\n",
    "        self.num_tokens = len(self.counts)\n",
    "        nbprint('Removed {} tokens occuring in less than {} documents'\n",
    "              .format(num_removed, min_docs))\n",
    "if RUN_SCRIPT:\n",
    "    with nbbox():\n",
    "        default_vocab_builder.filter_min_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `def filter_max_docs()`\n",
    "Remove tokens occuring in more than `max_docs` documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultVocabBuilder)\n",
    "def filter_max_docs(self):\n",
    "    if self.max_docs:\n",
    "        max_docs = self.to_abs(self.max_docs)\n",
    "        self.counts = [vocab_item \n",
    "                       for vocab_item in self.counts\n",
    "                       if vocab_item.document <= max_docs]\n",
    "        num_removed = self.num_tokens - len(self.counts)\n",
    "        self.num_tokens = len(self.counts)\n",
    "        nbprint('Removed {} tokens occuring in more than {} documents'\n",
    "              .format(num_removed, max_docs))\n",
    "if RUN_SCRIPT:\n",
    "    with nbbox():\n",
    "        default_vocab_builder.filter_max_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `def filter_min_count()`\n",
    "Remove tokens occuring less than `min_count` times in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultVocabBuilder)\n",
    "def filter_min_count(self):\n",
    "    if self.min_count:\n",
    "        min_count = self.to_abs(self.min_count)\n",
    "        self.counts = [vocab_item \n",
    "                       for vocab_item in self.counts\n",
    "                       if vocab_item.total >= min_count]\n",
    "        num_removed = self.num_tokens - len(self.counts)\n",
    "        self.num_tokens = len(self.counts)\n",
    "        nbprint('Removed {} tokens occuring less than {} times in total.'\n",
    "              .format(num_removed, min_count))\n",
    "if RUN_SCRIPT:\n",
    "    with nbbox():\n",
    "        default_vocab_builder.filter_min_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `def filter_max_count()`\n",
    "Remove tokens occuring less than `max_count` times in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultVocabBuilder)\n",
    "def filter_max_count(self):\n",
    "    if self.max_count:\n",
    "        max_count = self.to_abs(self.max_count)\n",
    "        self.counts = [vocab_item \n",
    "                       for vocab_item in self.counts\n",
    "                       if vocab_item.total >= max_count]\n",
    "        num_removed = self.num_tokens - len(self.counts)\n",
    "        self.num_tokens = len(self.counts)\n",
    "        nbprint('Removed {} tokens occuring more than {} times in total.'\n",
    "              .format(num_removed, max_count))\n",
    "if RUN_SCRIPT:\n",
    "    with nbbox():\n",
    "        default_vocab_builder.filter_max_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `def filter_min_word_length()`\n",
    "Remove tokens of length less than `min_word_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultVocabBuilder)\n",
    "def filter_min_word_length(self):\n",
    "    if self.min_word_length:\n",
    "        self.counts = [vocab_item\n",
    "                       for vocab_item in self.counts\n",
    "                       if len(vocab_item.token) >= self.min_word_length]\n",
    "        num_removed = self.num_tokens - len(self.counts)\n",
    "        self.num_tokens = len(self.counts)\n",
    "        nbprint('Removed {} tokens with length less than {}.'\n",
    "              .format(num_removed, self.min_word_length))\n",
    "if RUN_SCRIPT:\n",
    "    with nbbox():\n",
    "        default_vocab_builder.filter_min_word_length()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `def filter_max_word_length()`\n",
    "Remove tokens of length greater than `max_word_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultVocabBuilder)\n",
    "def filter_max_word_length(self):\n",
    "    if self.max_word_length:\n",
    "        self.counts = [vocab_item\n",
    "                       for vocab_item in self.counts\n",
    "                       if len(vocab_item.token) <= self.max_word_length]\n",
    "        num_removed = self.num_tokens - len(self.counts)\n",
    "        self.num_tokens = len(self.counts)\n",
    "        nbprint('Removed {} tokens with length greater than {}'\n",
    "              .format(num_removed, self.max_word_length))\n",
    "if RUN_SCRIPT:\n",
    "    with nbbox():\n",
    "        default_vocab_builder.filter_max_word_length()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `def filter_stopwords()`\n",
    "Remove tokens that are in the nltk stopword corpus `stopwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultVocabBuilder)\n",
    "def filter_stopwords(self):\n",
    "    if self.stopwords:\n",
    "        stopword_corpus = nltk_stopwords[self.stopwords]\n",
    "        self.counts = [vocab_item \n",
    "                       for vocab_item in self.counts\n",
    "                       if vocab_item.token not in stopword_corpus]\n",
    "        num_removed = self.num_tokens - len(self.counts)\n",
    "        self.num_tokens = len(self.counts)\n",
    "        nbprint('Removed {} tokens in the {} stopword corpus'\n",
    "              .format(num_removed, self.stopwords))\n",
    "if RUN_SCRIPT:\n",
    "    with nbbox():\n",
    "        default_vocab_builder.filter_stopwords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `def filter_total_size()`\n",
    "Remove tokens until the vocabulary is shorter than `max_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultVocabBuilder)\n",
    "def filter_total_size(self):\n",
    "    if self.max_tokens:\n",
    "        self.counts = self.counts[:self.max_tokens]\n",
    "        num_removed = self.num_tokens - len(self.counts)\n",
    "        self.num_tokens = len(self.counts)\n",
    "        nbprint('Removed {} tokens to limit vocabulary size to {}'\n",
    "              .format(num_removed, self.max_tokens))\n",
    "if RUN_SCRIPT:\n",
    "    with nbbox():\n",
    "        default_vocab_builder.filter_total_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Complete function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(DefaultVocabBuilder)\n",
    "def build_vocab(self):\n",
    "    self.count_tokens()\n",
    "    self.sort_counts()\n",
    "    self.filter_min_docs()\n",
    "    self.filter_max_docs()\n",
    "    self.filter_min_count()\n",
    "    self.filter_max_count()\n",
    "    self.filter_min_word_length()\n",
    "    self.filter_max_word_length()\n",
    "    self.filter_stopwords()\n",
    "    self.filter_total_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SCRIPT:\n",
    "    with nbbox():\n",
    "        default_vocab_builder.build_vocab()\n",
    "    show_tokens(default_vocab_builder.counts,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
