{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize cBoW\n",
    "<div style=\"position: absolute; right:0;top:0\"><a href=\"./vectorizer_index.doc.ipynb\" style=\"text-decoration: none\"> <font size=\"5\">←</font></a>\n",
    "<a href=\"../evaluation.ipynb\" style=\"text-decoration: none\"> <font size=\"5\">↑</font></a></div>\n",
    "\n",
    "`Description`\n",
    "\n",
    "---\n",
    "## Setup and Settings\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __init__ import init_vars\n",
    "init_vars(vars(), ('info', {}), ('runvars', {}), ('num_docs', 400))\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import data\n",
    "import config\n",
    "from base import nbprint\n",
    "from util import ProgressIterator\n",
    "from widgetbase import nbbox\n",
    "\n",
    "from embedding.main import get_model\n",
    "from embedding.common import OOVException\n",
    "\n",
    "from vectorizer.widgets import cbow_vector_picker\n",
    "from vectorizer.plots import plot_matrix\n",
    "from vectorizer.vectorize_bow import make_term_doc_mat_count, make_term_doc_mat_tf_idf\n",
    "\n",
    "if RUN_SCRIPT: cbow_vector_picker(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Count\n",
    "---\n",
    "\n",
    "Count matrix is the base for all further types. Take function `make_term_doc_mat_count()` from [Vectorize BoW](./vectorize_bow.ipynb#Count-Tokens-and-build-matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SCRIPT:\n",
    "    nbbox(mini=True)\n",
    "    make_term_doc_mat_count(info, runvars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tf-Idf\n",
    "---\n",
    "This matrix can be computed by multiplying the embedding matrix $V$, where each column is the wordembedding vector of the corresponding word, with the standard BoW tf-idf matrix $X$. $X$ is computed using the `make_term_doc_mat_tf_idf()` function from [Vectorize BoW](./vectorize_bow.ipynb#Build-complete-tokenizer-function). Note that $V$ will not be explicitly computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cbow_mat_tf_idf(info, runvars):\n",
    "    # Create tf-idf matrix\n",
    "    make_term_doc_mat_tf_idf(info, runvars)\n",
    "    tf_idf_mat = runvars['term_doc_mat_tf_idf']\n",
    "    \n",
    "    # Load vocabulary and wordembedding vectors\n",
    "    vocab_list = data.load_vocab_list(info)\n",
    "    model = get_model(info)\n",
    "    embedding_function = model.embedding_function\n",
    "    \n",
    "    # Create a zero matrix\n",
    "    cbow_tf_idf_shape = (model.vector_size, tf_idf_mat.shape[1])\n",
    "    cbow_tf_idf = np.zeros(cbow_tf_idf_shape)\n",
    "    \n",
    "    # Iterate over all nonzero entries of the tf-idf matrix:\n",
    "    nonzeros = zip(*sparse.find(tf_idf_mat))\n",
    "    for token_idx, doc_idx, value in ProgressIterator(nonzeros, length = tf_idf_mat.nnz, print_every = 5000):\n",
    "        # Add each entry times the corresponding vector to the matrix\n",
    "        try:\n",
    "            cbow_tf_idf[:,doc_idx] = cbow_tf_idf[:,doc_idx] + value * embedding_function(vocab_list[token_idx])\n",
    "        except OOVException:\n",
    "            pass\n",
    "        \n",
    "    # Return the matrix\n",
    "    runvars['cbow_mat'] = cbow_tf_idf\n",
    "\n",
    "if RUN_SCRIPT and info['vector_info']['type'] == 'TfIdf':\n",
    "    nbbox(mini=True)\n",
    "    make_cbow_mat_tf_idf(info, runvars)\n",
    "    plot_matrix(runvars['cbow_mat'][:,1:num_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## MinMaxMean\n",
    "---\n",
    "The MinMaxMean vector $x_i$ of a document is defined as $x_i = (x_i^{min}, x_i^{max}, x_i^{mean})^T$ with\n",
    "\n",
    "$$x_i^{min} = \\min (\\{v_j \\, \\vert \\, \\forall \\, \\text{tokens } j \\in \\text{document } i\\}),$$  \n",
    "$$x_i^{max} = \\max (\\{v_j \\, \\vert \\, \\forall \\, \\text{tokens } j \\in \\text{document } i\\}),$$  \n",
    "$$x_i^{mean} = \\frac{1}{\\text{number of tokens}} \\sum_j v_j,$$  \n",
    "where $\\min$ and $\\max$ are defined entry wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cbow_mat_minmaxmean(info, runvars):\n",
    "    # Get count matrix\n",
    "    count_mat = runvars['term_doc_mat_count']\n",
    "    \n",
    "    # Load vocabulary and wordembedding vectors\n",
    "    vocab_list = data.load_vocab_list(info)\n",
    "    model = get_model(info)\n",
    "    embedding_function = model.embedding_function\n",
    "    \n",
    "    # Create a zero matrix\n",
    "    cbow_m_shape = (model.vector_size, count_mat.shape[1])\n",
    "    cbow_min = np.full(cbow_m_shape, np.inf)\n",
    "    cbow_max = np.full(cbow_m_shape, -np.inf)\n",
    "    cbow_mean = np.zeros(cbow_m_shape)\n",
    "    column_sum = np.zeros(count_mat.shape[1])\n",
    "    \n",
    "    # Iterate over all nonzero entries of the count matrix:\n",
    "    nonzeros = zip(*sparse.find(count_mat))\n",
    "    for token_idx, doc_idx, value in ProgressIterator(nonzeros, length = count_mat.nnz, print_every = 5000):\n",
    "        try:\n",
    "            embedding_vector = embedding_function(vocab_list[token_idx])\n",
    "        except OOVException:\n",
    "            continue\n",
    "        # Entry wise minimum with the embedding vector\n",
    "        cbow_min[:,doc_idx] = np.minimum(cbow_min[:,doc_idx], embedding_vector)\n",
    "        # Entry wise maximum with the embedding vector\n",
    "        cbow_max[:,doc_idx] = np.maximum(cbow_max[:,doc_idx], embedding_vector)\n",
    "        # Sum up all embedding vectors and the total number of tokens in the document\n",
    "        cbow_mean[:,doc_idx] = cbow_mean[:,doc_idx] + value * embedding_vector\n",
    "        column_sum[doc_idx] = column_sum[doc_idx] + value\n",
    "        \n",
    "    # Divide sum by number of tokens\n",
    "    cbow_mean = cbow_mean * sparse.diags(1/np.maximum(1, column_sum))\n",
    "    \n",
    "    # Stack all matrices and return\n",
    "    cbow_mat = np.vstack((cbow_min,cbow_max,cbow_mean))\n",
    "    cbow_mat[np.invert(np.isfinite(cbow_mat))] = 0\n",
    "    runvars['cbow_mat'] = cbow_mat\n",
    "\n",
    "if RUN_SCRIPT and info['vector_info']['type'] == 'MinMaxMean':\n",
    "    nbbox(mini=True)\n",
    "    make_cbow_mat_minmaxmean(info, runvars)\n",
    "    plot_matrix(runvars['cbow_mat'][:,1:num_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Fisher Vector\n",
    "---\n",
    "Compute mean and variance of all vectors in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fv_mean_var_vectors(info, runvars):\n",
    "    # Load vocabulary and wordembedding vectors\n",
    "    vocab_list = data.load_vocab_list(info)\n",
    "    model = get_model(info)\n",
    "    embedding_function = model.embedding_function\n",
    "    vec_shape = model.vector_size\n",
    "    \n",
    "    # Sum to compute mean\n",
    "    mean_vec = np.zeros(vec_shape)\n",
    "    count = 0\n",
    "    for token in vocab_list:\n",
    "        try:\n",
    "            mean_vec += embedding_function(token)\n",
    "            count += 1\n",
    "        except OOVException:\n",
    "            continue\n",
    "    if count > 0:\n",
    "        mean_vec /= count\n",
    "    runvars['mean_vec'] = mean_vec\n",
    "    \n",
    "    # Sum to compute variance\n",
    "    var_vec = np.zeros(vec_shape)\n",
    "    count = 0\n",
    "    for token in vocab_list:\n",
    "        try:\n",
    "            var_vec += np.square(embedding_function(token) - mean_vec)\n",
    "            count += 1\n",
    "        except OOVException:\n",
    "            continue\n",
    "    if count > 0:\n",
    "        var_vec /= count\n",
    "    runvars['var_vec'] = np.maximum(0.001, var_vec)\n",
    "\n",
    "if RUN_SCRIPT and info['vector_info']['type'] == 'FV':\n",
    "    nbbox(mini=True)\n",
    "    fv_mean_var_vectors(info, runvars)\n",
    "    nbprint('Mean: {}...'.format(runvars['mean_vec'][:10]))\n",
    "    nbprint('Variance: {}...'.format(runvars['var_vec'][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the FVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fv_build_mat(info, runvars):    \n",
    "    # Get matrices\n",
    "    count_mat = runvars['term_doc_mat_count']\n",
    "    mean_vec = runvars['mean_vec']\n",
    "    var_vec = runvars['var_vec']\n",
    "    \n",
    "    # Load vocabulary and wordembedding vectors\n",
    "    vocab_list = data.load_vocab_list(info)\n",
    "    model = get_model(info)\n",
    "    embedding_function = model.embedding_function\n",
    "    \n",
    "    # Create a zero matrix\n",
    "    dimension = model.vector_size\n",
    "    fv_m_shape = (dimension*2, count_mat.shape[1])\n",
    "    fv_mat = np.zeros(fv_m_shape)\n",
    "    fv_num_tokens_shape = (1, count_mat.shape[1])\n",
    "    fv_num_tokens = np.zeros(fv_num_tokens_shape)\n",
    "    \n",
    "    # iterate all nonzero entries\n",
    "    nonzeros = zip(*sparse.find(count_mat))\n",
    "    for token_idx, doc_idx, value in ProgressIterator(nonzeros, length = count_mat.nnz, print_every = 5000):\n",
    "        try:\n",
    "            embedding_vector = embedding_function(vocab_list[token_idx])\n",
    "        except OOVException:\n",
    "            continue\n",
    "        fv_mat[:dimension, doc_idx] += value * (embedding_vector - mean_vec) / var_vec\n",
    "        fv_mat[dimension:, doc_idx] += value * (np.square(embedding_vector - mean_vec) / (var_vec * np.sqrt(var_vec)) - (1 / np.sqrt(var_vec)))\n",
    "        fv_num_tokens[0,doc_idx] += value\n",
    "        \n",
    "    # normalize\n",
    "    fv_num_tokens[fv_num_tokens == 0] = 1\n",
    "    fv_mat *= np.power(fv_num_tokens, -0.5)\n",
    "    fv_mat[:dimension,:] = (fv_mat[:dimension,:].transpose() * np.nan_to_num(np.power(1 / var_vec, -0.5))).transpose()\n",
    "    fv_mat[dimension:,:] = (fv_mat[dimension:,:].transpose() * np.nan_to_num(np.power(2 / var_vec, -0.5))).transpose()\n",
    "        \n",
    "    runvars['cbow_mat'] = fv_mat\n",
    "    \n",
    "if RUN_SCRIPT and info['vector_info']['type'] == 'FV':\n",
    "    nbbox(mini=True)\n",
    "    fv_build_mat(info, runvars)\n",
    "    plot_matrix(runvars['cbow_mat'][:,1:num_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make complete function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cbow_fv(info, runvars):\n",
    "    fv_mean_var_vectors(info, runvars)\n",
    "    fv_build_mat(info, runvars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
